{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MODEL[2].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"3XI3zjLLq5iB","colab_type":"code","colab":{},"outputId":"c2d4ee86-dffa-4206-ea9a-1e19779a393d"},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import print_function\n","import sys\n","#reload(sys)\n","#sys.setdefaultencoding('utf-8')\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import re\n","import itertools\n","import os.path\n","import json\n","from datetime import datetime"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"CgehrVFlq5iK","colab_type":"code","colab":{}},"cell_type":"code","source":["def k_to_one_hot(k_hot_vector):\n","    # This function converts k-hot target vector to one-hot target matrix\n","    \n","    classes = np.unique(k_hot_vector)\n","    one_hot_matrix = []\n","    \n","    for i in np.arange(len(classes)):\n","        row = (k_hot_vector == classes[i]).astype(int, copy = False)\n","        if len(one_hot_matrix) == 0:\n","            one_hot_matrix = row\n","        else:\n","            one_hot_matrix = np.vstack((one_hot_matrix, row))\n","            \n","    return classes, one_hot_matrix.conj().transpose()\n","    \n","    \n","def read_data(filename):\n","    # read data into lists\n","    \n","    with open(filename) as data_file:    \n","        data = json.load(data_file)\n","        \n","    ids, cuisines, ingredients = [], [], []\n","    if 'cuisine' in data[0].keys():\n","        for i in range(len(data)):\n","            ids.append(data[i]['id'])\n","            cuisines.append(data[i]['cuisine'])\n","            ingredients.append(data[i]['ingredients'])\n","    else:\n","        for i in range(len(data)):\n","            ids.append(data[i]['id'])\n","            ingredients.append(data[i]['ingredients'])    \n","                \n","    return ids, cuisines, ingredients\n","    \n","    \n","def create_submission(test_ids, guess):\n","    # create submission in proper format\n","    \n","    sub = np.transpose(np.vstack((test_ids, guess)))\n","    sub = np.vstack((['id', 'cuisine'], sub))\n","    sub_file_name = './submission_' + str(datetime.now())[0:16] +'.csv'\n","    sub_file_name = sub_file_name.replace(' ', '_')\n","    sub_file_name = sub_file_name.replace(':', '-')\n","    np.savetxt(sub_file_name, sub, delimiter=\",\", fmt=\"%s\")\n","    \n","    return None  \n","    \n","\n","def remove_numbers(ing):\n","    # remove numbers from ingredients\n","    \n","    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in ing]\n","\n","    \n","def remove_special_chars(ing):\n","    # remove certain special characters from ingredients\n","   \n","    ing = [[x.replace(\"-\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"&\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"'\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"''\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"%\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"!\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"(\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\")\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\",\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(\".\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in ing] \n","    ing = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in ing] \n","\n","    return ing\n","    \n","    \n","def make_lowercase(ing):\n","    # make all letters lowercase for all ingredients\n","    \n","    return [[x.lower() for x in y] for y in ing]\n","    \n","    \n","def remove_extra_whitespace(ing):\n","    # removes extra whitespaces\n","    \n","    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in ing] \n","    \n","    \n","def stem_words(ing):\n","    # word stemming for ingredients\n","    \n","    lmtzr = WordNetLemmatizer()\n","    \n","    def word_by_word(strng):\n","        \n","        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in strng.split()])\n","    \n","    return [[word_by_word(x) for x in y] for y in ing] \n","    \n","    \n","def remove_units(ing):\n","    # remove certain words from ingredients\n","    \n","    remove_list = ['g', 'lb', 's', 'n']\n","        \n","    def check_word(strng):\n","        \n","        s = strng.split()\n","        resw  = [word for word in s if word.lower() not in remove_list]\n","        \n","        return ' '.join(resw)\n","\n","    return [[check_word(x) for x in y] for y in ing] \n","    \n","\n","def extract_feats(ingredients, uniques):\n","    # each ingredient + each word as feature\n","    \n","    feats_whole = np.zeros((len(ingredients), len(uniques)))\n","    for i in range(len(ingredients)):\n","        for j in ingredients[i]:\n","            feats_whole[i, uniques.index(j)] = 1\n","            \n","    new_uniques = []\n","    for m in uniques:\n","        new_uniques.append(m.split())\n","    new_uniques = list(set(list(itertools.chain.from_iterable(new_uniques))))\n","    \n","    feats_each = np.zeros((len(ingredients), len(new_uniques))).astype(np.uint8)\n","    for i in range(len(ingredients)):\n","        for j in ingredients[i]:\n","            for k in j.split():\n","                feats_each[i, new_uniques.index(k)] = 1\n","            \n","    return np.hstack((feats_whole, feats_each)).astype(bool)\n","    \n","    \n","def load_model():\n","    # load neural net model architectiure\n","    \n","    mdl = Sequential()\n","    mdl.add(Dense(512, init='glorot_uniform', activation='relu', input_shape=(train_feats.shape[1],)))\n","    mdl.add(Dropout(0.5))\n","    mdl.add(Dense(128, init='glorot_uniform', activation='relu'))\n","    mdl.add(Dropout(0.5))\n","    mdl.add(Dense(20, activation='softmax'))\n","    mdl.compile(loss='categorical_crossentropy', optimizer='adadelta')\n","    \n","    return mdl    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eJ5p1uG-q5iP","colab_type":"code","colab":{},"outputId":"8b63185e-d639-4cab-c2f6-bb2d590e6fcf"},"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"-IuXqacMq5iU","colab_type":"code","colab":{}},"cell_type":"code","source":["df=pd.read_json('./train.json')\n","#df['ingredients_train'] = df['ingredients'].apply(','.join)\n","X =  df['ingredients'].values\n","Y = df['cuisine'].values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZtsnaK_uq5iZ","colab_type":"code","colab":{},"outputId":"20759545-4d7b-4e72-e064-fd5da12148ff"},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2 ,random_state=2019, stratify = Y)\n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(31819,) (7955,) (31819,) (7955,)\n"],"name":"stdout"}]},{"metadata":{"id":"LM6XH0Ggq5id","colab_type":"code","colab":{}},"cell_type":"code","source":["# train_ids, train_cuisines, train_ingredients = read_data('train.json')\n","X_train = make_lowercase(X_train)\n","X_train = remove_numbers(X_train)\n","X_train = remove_special_chars(X_train)\n","X_train = remove_extra_whitespace(X_train)\n","X_train = remove_units(X_train)\n","X_train = stem_words(X_train)\n","\n","# preprocess test set\n","# test_ids, test_cuisines, test_ingredients = read_data('test.json')\n","X_test = make_lowercase(X_test)\n","X_test = remove_numbers(X_test)\n","X_test = remove_special_chars(X_test)\n","X_test = remove_extra_whitespace(X_test)\n","X_test = remove_units(X_test)\n","X_test = stem_words(X_test)\n","\n","\n","# train_cuisines = y_train\n","\n","le = LabelEncoder()\n","targets = le.fit_transform(y_train)\n","classes, targets = k_to_one_hot(targets)\n","\n","# extract features\n","\n","uniques = list(set([item for sublist in X_train + X_test for item in sublist]))\n","train_feats = extract_feats(X_train, uniques)\n","test_feats = extract_feats(X_test, uniques)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uskXsPbsq5ih","colab_type":"code","colab":{}},"cell_type":"code","source":["n_ensemble = 10\n","for ens in range(n_ensemble):\n","    print(\"\\n\\tTraining...\", ens)\n","    model = load_model()\n","\n","    # if model already exists, continue training\n","    model_name = 'model' + str(ens) + '.hdf5'\n","    if os.path.isfile(model_name):\n","        model.load_weights(model_name)\n","\n","#    model.fit(train_feats, targets, nb_epoch=2500, batch_size=4096)\n","    model.fit(train_feats, targets, nb_epoch=2000, batch_size=4096)\n","    model.save_weights(model_name, overwrite=True)\n","\n","# create submission out of the ensemble\n","preds = []\n","for ens in range(n_ensemble):\n","    print(\"\\nSubmission\", ens)\n","    model = load_model()\n","\n","    model_name = 'model' + str(ens) + '.hdf5'\n","    model.load_weights(model_name)            \n","    preds.append(model.predict_proba(test_feats))\n","\n","# final cuisine decision: argmax of sum of log probabilities  \n","print(\"\\nPredicting...\")      \n","preds = sum(np.log(preds))\n","guess = le.inverse_transform(np.argmax(preds, axis=1))\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Co534K1pq5it","colab_type":"code","colab":{},"outputId":"d8e8bdf0-237c-4dee-8f69-f9d341289e5c"},"cell_type":"code","source":["# create submission out of the ensemble\n","n_ensemble = 10\n","preds = []\n","for ens in range(n_ensemble):\n","    print(\"\\nSubmission\", ens)\n","    model = load_model()\n","\n","    model_name = './2000ep/model' + str(ens) + '.hdf5'\n","    model.load_weights(model_name)            \n","    preds.append(model.predict_proba(test_feats))\n","\n","# final cuisine decision: argmax of sum of log probabilities  \n","print(\"\\nPredicting...\")      \n","preds = sum(np.log(preds))\n","guess = le.inverse_transform(np.argmax(preds, axis=1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Submission 0\n"],"name":"stdout"},{"output_type":"stream","text":["/home/ncp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:143: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", input_shape=(9452,), kernel_initializer=\"glorot_uniform\")`\n","/home/ncp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:145: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Submission 1\n","\n","Submission 2\n","\n","Submission 3\n","\n","Submission 4\n","\n","Submission 5\n","\n","Submission 6\n","\n","Submission 7\n","\n","Submission 8\n","\n","Submission 9\n","\n","Predicting...\n"],"name":"stdout"},{"output_type":"stream","text":["/home/ncp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n","  \n"],"name":"stderr"}]},{"metadata":{"id":"PdKnekF2q5iy","colab_type":"code","colab":{},"outputId":"e545f3b3-788b-4884-cf6f-b2c5390946ef"},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, guess))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   brazilian       0.82      0.65      0.72        93\n","     british       0.67      0.57      0.61       161\n","cajun_creole       0.79      0.72      0.75       309\n","     chinese       0.82      0.87      0.84       535\n","    filipino       0.74      0.68      0.71       151\n","      french       0.68      0.70      0.69       529\n","       greek       0.82      0.76      0.79       235\n","      indian       0.88      0.92      0.90       601\n","       irish       0.69      0.54      0.61       133\n","     italian       0.84      0.91      0.87      1568\n","    jamaican       0.87      0.73      0.79       105\n","    japanese       0.84      0.75      0.79       284\n","      korean       0.84      0.78      0.81       166\n","     mexican       0.92      0.93      0.92      1288\n","    moroccan       0.84      0.84      0.84       164\n","     russian       0.70      0.53      0.60        98\n"," southern_us       0.78      0.82      0.80       864\n","     spanish       0.69      0.57      0.62       198\n","        thai       0.84      0.85      0.84       308\n","  vietnamese       0.77      0.69      0.73       165\n","\n","   micro avg       0.82      0.82      0.82      7955\n","   macro avg       0.79      0.74      0.76      7955\n","weighted avg       0.82      0.82      0.82      7955\n","\n"],"name":"stdout"}]},{"metadata":{"id":"UMDeY0o-q5i_","colab_type":"code","colab":{},"outputId":"c4aade48-49e6-48b7-e6d9-e537332e6419"},"cell_type":"code","source":["from sklearn import metrics\n","\n","score = metrics.accuracy_score(y_test, guess)\n","print(\"accuracy:   %0.3f\" % score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracy:   0.822\n"],"name":"stdout"}]}]}