{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of\n",
    "\n",
    "\n",
    "MODEL_1 and 3 <br>\n",
    "CLASSIFIED WITH WORDS and PHRASE<br>\n",
    "ngram = 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import itertools\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(ing):\n",
    "    # remove numbers from ingredients\n",
    "    \n",
    "    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in ing]\n",
    "\n",
    "    \n",
    "def remove_special_chars(ing):\n",
    "    # remove certain special characters from ingredients\n",
    "   \n",
    "    ing = [[x.replace(\"-\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"&\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"'\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"''\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"%\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"!\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"(\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\")\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\",\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\".\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in ing] \n",
    "\n",
    "    return ing\n",
    "    \n",
    "    \n",
    "def make_lowercase(ing):\n",
    "    # make all letters lowercase for all ingredients\n",
    "    \n",
    "    return [[x.lower() for x in y] for y in ing]\n",
    "    \n",
    "    \n",
    "def remove_extra_whitespace(ing):\n",
    "    # removes extra whitespaces\n",
    "    \n",
    "    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in ing] \n",
    "    \n",
    "    \n",
    "def stem_words(ing):\n",
    "    # word stemming for ingredients\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    def word_by_word(strng):\n",
    "        \n",
    "        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in strng.split()])\n",
    "    \n",
    "    return [[word_by_word(x) for x in y] for y in ing] \n",
    "    \n",
    "    \n",
    "def remove_units(ing):\n",
    "    # remove certain words from ingredients\n",
    "    \n",
    "    remove_list = ['g', 'lb', 's', 'n']\n",
    "        \n",
    "    def check_word(strng):\n",
    "        \n",
    "        s = strng.split()\n",
    "        resw  = [word for word in s if word.lower() not in remove_list]\n",
    "        \n",
    "        return ' '.join(resw)\n",
    "\n",
    "    return [[check_word(x) for x in y] for y in ing] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### by all the words\n",
    "\n",
    "df=pd.read_json('./train.json')\n",
    "X = df['ingredients'].values\n",
    "Y = df['cuisine'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_lowercase(X)\n",
    "X = remove_numbers(X)\n",
    "X = remove_special_chars(X)\n",
    "X = remove_extra_whitespace(X)\n",
    "X = remove_units(X)\n",
    "X = stem_words(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients_preprocessed'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients_as_sentence'] = df['ingredients_preprocessed'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6677\n"
     ]
    }
   ],
   "source": [
    "uniques = list(set([item for sublist in X for item in sublist]))\n",
    "print(len(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['ingredients_as_sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8285\n"
     ]
    }
   ],
   "source": [
    "for ing in uniques:\n",
    "    temp = ing.split(' ')\n",
    "    for word in temp:\n",
    "        if word not in uniques:\n",
    "            uniques.append(word)\n",
    "print(len(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 1     #### CUSTOMIZE \n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, ngram), vocabulary=uniques)\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7434)\t0.4043455766076157\n",
      "  (0, 7412)\t0.3167276767158082\n",
      "  (0, 7058)\t0.24911439037394897\n",
      "  (0, 6765)\t0.35697121618985855\n",
      "  (0, 6533)\t0.14805335445131518\n",
      "  (0, 5445)\t0.10970675831861157\n",
      "  (0, 4551)\t0.2774555999091271\n",
      "  (0, 4306)\t0.15176502988889437\n",
      "  (0, 3811)\t0.2396832355219728\n",
      "  (0, 3570)\t0.1118145095283498\n",
      "  (0, 3272)\t0.10391647275486823\n",
      "  (0, 3094)\t0.3560443893695225\n",
      "  (0, 2952)\t0.1457359419289999\n",
      "  (0, 2283)\t0.34827614404150986\n",
      "  (0, 1263)\t0.20790510180038077\n",
      "  (0, 122)\t0.13948352921322324\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 8285) (7955, 8285) (31819,) (7955,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2 ,random_state=2019, stratify = Y )\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \n",
    "    \n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "        \n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,target_names=target_names))\n",
    "    \n",
    "    \n",
    "    #if opts.print_cm:\n",
    "    #print(\"confusion matrix:\")\n",
    "    #print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=2019, solver='sag',\n",
      "        tol=0.01)\n",
      "train time: 2.699s\n",
      "test time:  0.003s\n",
      "accuracy:   0.764\n",
      "dimensionality: 8285\n",
      "density: 0.322028\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.75      0.43      0.55        93\n",
      "     british       0.74      0.30      0.43       161\n",
      "cajun_creole       0.79      0.65      0.72       309\n",
      "     chinese       0.72      0.88      0.80       535\n",
      "    filipino       0.73      0.43      0.54       151\n",
      "      french       0.61      0.66      0.63       529\n",
      "       greek       0.82      0.68      0.74       235\n",
      "      indian       0.83      0.89      0.86       601\n",
      "       irish       0.69      0.29      0.40       133\n",
      "     italian       0.76      0.91      0.83      1568\n",
      "    jamaican       0.73      0.54      0.62       105\n",
      "    japanese       0.87      0.67      0.76       284\n",
      "      korean       0.85      0.61      0.71       166\n",
      "     mexican       0.88      0.92      0.90      1288\n",
      "    moroccan       0.83      0.67      0.74       164\n",
      "     russian       0.75      0.34      0.46        98\n",
      " southern_us       0.66      0.80      0.72       864\n",
      "     spanish       0.75      0.36      0.49       198\n",
      "        thai       0.72      0.77      0.75       308\n",
      "  vietnamese       0.73      0.44      0.55       165\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      7955\n",
      "   macro avg       0.76      0.61      0.66      7955\n",
      "weighted avg       0.77      0.76      0.75      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "      fit_intercept=True, max_iter=50, n_iter=None, n_iter_no_change=5,\n",
      "      n_jobs=None, penalty=None, random_state=2019, shuffle=True,\n",
      "      tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.355s\n",
      "test time:  0.003s\n",
      "accuracy:   0.733\n",
      "dimensionality: 8285\n",
      "density: 0.141002\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.48      0.61      0.54        93\n",
      "     british       0.40      0.34      0.37       161\n",
      "cajun_creole       0.57      0.67      0.62       309\n",
      "     chinese       0.72      0.85      0.78       535\n",
      "    filipino       0.61      0.50      0.55       151\n",
      "      french       0.54      0.61      0.57       529\n",
      "       greek       0.73      0.69      0.70       235\n",
      "      indian       0.82      0.89      0.85       601\n",
      "       irish       0.62      0.37      0.46       133\n",
      "     italian       0.84      0.78      0.81      1568\n",
      "    jamaican       0.59      0.68      0.63       105\n",
      "    japanese       0.80      0.64      0.71       284\n",
      "      korean       0.62      0.80      0.69       166\n",
      "     mexican       0.88      0.90      0.89      1288\n",
      "    moroccan       0.76      0.69      0.72       164\n",
      "     russian       0.45      0.35      0.39        98\n",
      " southern_us       0.65      0.73      0.69       864\n",
      "     spanish       0.64      0.44      0.52       198\n",
      "        thai       0.82      0.66      0.73       308\n",
      "  vietnamese       0.71      0.50      0.58       165\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      7955\n",
      "   macro avg       0.66      0.63      0.64      7955\n",
      "weighted avg       0.74      0.73      0.73      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
      "              max_iter=50, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "              random_state=2019, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.770s\n",
      "test time:  0.003s\n",
      "accuracy:   0.765\n",
      "dimensionality: 8285\n",
      "density: 0.206500\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.70      0.61      0.66        93\n",
      "     british       0.59      0.42      0.49       161\n",
      "cajun_creole       0.72      0.67      0.69       309\n",
      "     chinese       0.79      0.79      0.79       535\n",
      "    filipino       0.63      0.66      0.64       151\n",
      "      french       0.56      0.66      0.61       529\n",
      "       greek       0.76      0.74      0.75       235\n",
      "      indian       0.88      0.85      0.87       601\n",
      "       irish       0.53      0.38      0.44       133\n",
      "     italian       0.84      0.84      0.84      1568\n",
      "    jamaican       0.81      0.69      0.74       105\n",
      "    japanese       0.77      0.71      0.74       284\n",
      "      korean       0.75      0.82      0.78       166\n",
      "     mexican       0.89      0.91      0.90      1288\n",
      "    moroccan       0.74      0.73      0.74       164\n",
      "     russian       0.49      0.44      0.46        98\n",
      " southern_us       0.68      0.78      0.73       864\n",
      "     spanish       0.61      0.48      0.54       198\n",
      "        thai       0.77      0.75      0.76       308\n",
      "  vietnamese       0.65      0.54      0.59       165\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      7955\n",
      "   macro avg       0.71      0.67      0.69      7955\n",
      "weighted avg       0.77      0.77      0.76      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.044s\n",
      "test time:  10.028s\n",
      "accuracy:   0.753\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.60      0.54      0.56        93\n",
      "     british       0.63      0.43      0.51       161\n",
      "cajun_creole       0.69      0.67      0.68       309\n",
      "     chinese       0.72      0.87      0.78       535\n",
      "    filipino       0.67      0.58      0.62       151\n",
      "      french       0.56      0.61      0.59       529\n",
      "       greek       0.73      0.65      0.68       235\n",
      "      indian       0.87      0.88      0.87       601\n",
      "       irish       0.67      0.51      0.58       133\n",
      "     italian       0.75      0.88      0.81      1568\n",
      "    jamaican       0.83      0.57      0.68       105\n",
      "    japanese       0.87      0.65      0.75       284\n",
      "      korean       0.78      0.72      0.75       166\n",
      "     mexican       0.84      0.88      0.86      1288\n",
      "    moroccan       0.86      0.76      0.80       164\n",
      "     russian       0.71      0.30      0.42        98\n",
      " southern_us       0.72      0.74      0.73       864\n",
      "     spanish       0.68      0.35      0.46       198\n",
      "        thai       0.78      0.74      0.76       308\n",
      "  vietnamese       0.86      0.47      0.61       165\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      7955\n",
      "   macro avg       0.74      0.64      0.68      7955\n",
      "weighted avg       0.75      0.75      0.75      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=2019, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 44.484s\n",
      "test time:  0.312s\n",
      "accuracy:   0.754\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.88      0.39      0.54        93\n",
      "     british       0.82      0.25      0.38       161\n",
      "cajun_creole       0.78      0.64      0.71       309\n",
      "     chinese       0.73      0.89      0.80       535\n",
      "    filipino       0.80      0.46      0.58       151\n",
      "      french       0.60      0.53      0.56       529\n",
      "       greek       0.89      0.55      0.68       235\n",
      "      indian       0.82      0.92      0.87       601\n",
      "       irish       0.80      0.43      0.56       133\n",
      "     italian       0.70      0.93      0.80      1568\n",
      "    jamaican       0.96      0.48      0.64       105\n",
      "    japanese       0.87      0.64      0.74       284\n",
      "      korean       0.91      0.63      0.74       166\n",
      "     mexican       0.85      0.94      0.89      1288\n",
      "    moroccan       0.91      0.64      0.75       164\n",
      "     russian       0.77      0.23      0.36        98\n",
      " southern_us       0.65      0.77      0.70       864\n",
      "     spanish       0.85      0.31      0.45       198\n",
      "        thai       0.79      0.78      0.79       308\n",
      "  vietnamese       0.83      0.42      0.56       165\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      7955\n",
      "   macro avg       0.81      0.59      0.65      7955\n",
      "weighted avg       0.77      0.75      0.74      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.914s\n",
      "test time:  0.003s\n",
      "accuracy:   0.796\n",
      "dimensionality: 8285\n",
      "density: 0.322028\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.77      0.59      0.67        93\n",
      "     british       0.68      0.46      0.55       161\n",
      "cajun_creole       0.76      0.72      0.74       309\n",
      "     chinese       0.80      0.86      0.83       535\n",
      "    filipino       0.76      0.62      0.68       151\n",
      "      french       0.62      0.69      0.65       529\n",
      "       greek       0.81      0.72      0.77       235\n",
      "      indian       0.88      0.91      0.89       601\n",
      "       irish       0.65      0.45      0.53       133\n",
      "     italian       0.83      0.88      0.86      1568\n",
      "    jamaican       0.82      0.71      0.76       105\n",
      "    japanese       0.84      0.70      0.77       284\n",
      "      korean       0.82      0.80      0.81       166\n",
      "     mexican       0.90      0.93      0.91      1288\n",
      "    moroccan       0.80      0.79      0.79       164\n",
      "     russian       0.61      0.46      0.52        98\n",
      " southern_us       0.71      0.80      0.76       864\n",
      "     spanish       0.69      0.48      0.57       198\n",
      "        thai       0.79      0.79      0.79       308\n",
      "  vietnamese       0.72      0.60      0.66       165\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7955\n",
      "   macro avg       0.76      0.70      0.72      7955\n",
      "weighted avg       0.79      0.80      0.79      7955\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=2019, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2.473s\n",
      "test time:  0.003s\n",
      "accuracy:   0.781\n",
      "dimensionality: 8285\n",
      "density: 0.150398\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.74      0.49      0.59        93\n",
      "     british       0.69      0.27      0.39       161\n",
      "cajun_creole       0.76      0.70      0.73       309\n",
      "     chinese       0.78      0.88      0.83       535\n",
      "    filipino       0.74      0.50      0.60       151\n",
      "      french       0.65      0.63      0.64       529\n",
      "       greek       0.82      0.71      0.76       235\n",
      "      indian       0.83      0.91      0.87       601\n",
      "       irish       0.72      0.32      0.45       133\n",
      "     italian       0.79      0.91      0.85      1568\n",
      "    jamaican       0.82      0.68      0.74       105\n",
      "    japanese       0.82      0.68      0.74       284\n",
      "      korean       0.79      0.72      0.75       166\n",
      "     mexican       0.89      0.93      0.91      1288\n",
      "    moroccan       0.80      0.76      0.78       164\n",
      "     russian       0.60      0.42      0.49        98\n",
      " southern_us       0.67      0.79      0.73       864\n",
      "     spanish       0.74      0.42      0.54       198\n",
      "        thai       0.76      0.81      0.78       308\n",
      "  vietnamese       0.81      0.50      0.62       165\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      7955\n",
      "   macro avg       0.76      0.65      0.69      7955\n",
      "weighted avg       0.78      0.78      0.77      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 16.402s\n",
      "test time:  0.003s\n",
      "accuracy:   0.795\n",
      "dimensionality: 8285\n",
      "density: 0.067610\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.76      0.58      0.66        93\n",
      "     british       0.70      0.45      0.55       161\n",
      "cajun_creole       0.75      0.71      0.73       309\n",
      "     chinese       0.80      0.87      0.83       535\n",
      "    filipino       0.72      0.58      0.64       151\n",
      "      french       0.62      0.68      0.65       529\n",
      "       greek       0.81      0.72      0.76       235\n",
      "      indian       0.87      0.91      0.89       601\n",
      "       irish       0.67      0.44      0.53       133\n",
      "     italian       0.83      0.88      0.85      1568\n",
      "    jamaican       0.82      0.71      0.77       105\n",
      "    japanese       0.86      0.70      0.77       284\n",
      "      korean       0.82      0.79      0.80       166\n",
      "     mexican       0.91      0.93      0.92      1288\n",
      "    moroccan       0.82      0.80      0.81       164\n",
      "     russian       0.62      0.44      0.51        98\n",
      " southern_us       0.70      0.80      0.75       864\n",
      "     spanish       0.67      0.48      0.56       198\n",
      "        thai       0.80      0.80      0.80       308\n",
      "  vietnamese       0.73      0.58      0.65       165\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7955\n",
      "   macro avg       0.76      0.69      0.72      7955\n",
      "weighted avg       0.79      0.80      0.79      7955\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
      "       power_t=0.5, random_state=2019, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 5.531s\n",
      "test time:  0.003s\n",
      "accuracy:   0.732\n",
      "dimensionality: 8285\n",
      "density: 0.011503\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.15      0.35      0.21        93\n",
      "     british       0.93      0.09      0.16       161\n",
      "cajun_creole       0.74      0.66      0.69       309\n",
      "     chinese       0.72      0.87      0.79       535\n",
      "    filipino       0.82      0.21      0.34       151\n",
      "      french       0.63      0.49      0.55       529\n",
      "       greek       0.78      0.69      0.73       235\n",
      "      indian       0.82      0.90      0.86       601\n",
      "       irish       0.67      0.23      0.35       133\n",
      "     italian       0.76      0.90      0.82      1568\n",
      "    jamaican       0.76      0.55      0.64       105\n",
      "    japanese       0.60      0.65      0.62       284\n",
      "      korean       0.79      0.64      0.71       166\n",
      "     mexican       0.88      0.92      0.90      1288\n",
      "    moroccan       0.81      0.69      0.74       164\n",
      "     russian       0.50      0.14      0.22        98\n",
      " southern_us       0.64      0.78      0.70       864\n",
      "     spanish       0.56      0.19      0.28       198\n",
      "        thai       0.71      0.82      0.76       308\n",
      "  vietnamese       0.76      0.25      0.37       165\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      7955\n",
      "   macro avg       0.70      0.55      0.57      7955\n",
      "weighted avg       0.74      0.73      0.71      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=2019, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 7.095s\n",
      "test time:  0.003s\n",
      "accuracy:   0.775\n",
      "dimensionality: 8285\n",
      "density: 0.058950\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.60      0.43      0.50        93\n",
      "     british       0.54      0.20      0.30       161\n",
      "cajun_creole       0.75      0.69      0.72       309\n",
      "     chinese       0.77      0.88      0.82       535\n",
      "    filipino       0.73      0.47      0.57       151\n",
      "      french       0.64      0.62      0.63       529\n",
      "       greek       0.81      0.71      0.76       235\n",
      "      indian       0.83      0.92      0.87       601\n",
      "       irish       0.81      0.29      0.43       133\n",
      "     italian       0.79      0.91      0.84      1568\n",
      "    jamaican       0.84      0.68      0.75       105\n",
      "    japanese       0.85      0.67      0.75       284\n",
      "      korean       0.80      0.72      0.76       166\n",
      "     mexican       0.89      0.93      0.91      1288\n",
      "    moroccan       0.80      0.75      0.78       164\n",
      "     russian       0.61      0.34      0.43        98\n",
      " southern_us       0.66      0.78      0.72       864\n",
      "     spanish       0.72      0.39      0.50       198\n",
      "        thai       0.76      0.83      0.79       308\n",
      "  vietnamese       0.81      0.48      0.61       165\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      7955\n",
      "   macro avg       0.75      0.63      0.67      7955\n",
      "weighted avg       0.77      0.78      0.76      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.048s\n",
      "test time:  0.004s\n",
      "accuracy:   0.617\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.21      0.51      0.30        93\n",
      "     british       0.17      0.51      0.26       161\n",
      "cajun_creole       0.53      0.73      0.62       309\n",
      "     chinese       0.81      0.71      0.76       535\n",
      "    filipino       0.33      0.54      0.41       151\n",
      "      french       0.40      0.44      0.42       529\n",
      "       greek       0.55      0.65      0.60       235\n",
      "      indian       0.87      0.71      0.78       601\n",
      "       irish       0.35      0.59      0.44       133\n",
      "     italian       0.89      0.62      0.73      1568\n",
      "    jamaican       0.54      0.53      0.54       105\n",
      "    japanese       0.81      0.57      0.67       284\n",
      "      korean       0.53      0.63      0.58       166\n",
      "     mexican       0.94      0.75      0.84      1288\n",
      "    moroccan       0.52      0.79      0.63       164\n",
      "     russian       0.30      0.45      0.36        98\n",
      " southern_us       0.68      0.42      0.52       864\n",
      "     spanish       0.23      0.64      0.34       198\n",
      "        thai       0.74      0.59      0.66       308\n",
      "  vietnamese       0.46      0.62      0.53       165\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      7955\n",
      "   macro avg       0.54      0.60      0.55      7955\n",
      "weighted avg       0.71      0.62      0.64      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.130s\n",
      "test time:  0.003s\n",
      "accuracy:   0.739\n",
      "dimensionality: 8285\n",
      "density: 1.000000\n",
      "classification report:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.81      0.45      0.58        93\n",
      "     british       0.53      0.30      0.39       161\n",
      "cajun_creole       0.69      0.64      0.66       309\n",
      "     chinese       0.72      0.88      0.80       535\n",
      "    filipino       0.79      0.47      0.59       151\n",
      "      french       0.57      0.57      0.57       529\n",
      "       greek       0.87      0.54      0.67       235\n",
      "      indian       0.83      0.89      0.86       601\n",
      "       irish       0.71      0.26      0.38       133\n",
      "     italian       0.75      0.88      0.81      1568\n",
      "    jamaican       0.88      0.50      0.63       105\n",
      "    japanese       0.89      0.62      0.73       284\n",
      "      korean       0.87      0.62      0.73       166\n",
      "     mexican       0.86      0.89      0.88      1288\n",
      "    moroccan       0.81      0.66      0.73       164\n",
      "     russian       0.68      0.31      0.42        98\n",
      " southern_us       0.57      0.75      0.65       864\n",
      "     spanish       0.72      0.36      0.48       198\n",
      "        thai       0.72      0.76      0.74       308\n",
      "  vietnamese       0.73      0.50      0.59       165\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      7955\n",
      "   macro avg       0.75      0.59      0.64      7955\n",
      "weighted avg       0.75      0.74      0.73      7955\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.132s\n",
      "test time:  0.009s\n",
      "accuracy:   0.724\n",
      "dimensionality: 8285\n",
      "density: 1.000000\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.44      0.54      0.49        93\n",
      "     british       0.28      0.55      0.37       161\n",
      "cajun_creole       0.60      0.74      0.66       309\n",
      "     chinese       0.81      0.79      0.80       535\n",
      "    filipino       0.55      0.62      0.58       151\n",
      "      french       0.55      0.50      0.53       529\n",
      "       greek       0.68      0.69      0.69       235\n",
      "      indian       0.89      0.86      0.87       601\n",
      "       irish       0.51      0.56      0.53       133\n",
      "     italian       0.87      0.76      0.81      1568\n",
      "    jamaican       0.75      0.62      0.68       105\n",
      "    japanese       0.76      0.65      0.70       284\n",
      "      korean       0.74      0.75      0.74       166\n",
      "     mexican       0.93      0.85      0.89      1288\n",
      "    moroccan       0.72      0.74      0.73       164\n",
      "     russian       0.37      0.48      0.42        98\n",
      " southern_us       0.64      0.69      0.67       864\n",
      "     spanish       0.41      0.59      0.48       198\n",
      "        thai       0.77      0.70      0.73       308\n",
      "  vietnamese       0.57      0.56      0.56       165\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      7955\n",
      "   macro avg       0.64      0.66      0.65      7955\n",
      "weighted avg       0.75      0.72      0.73      7955\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)\n",
      "train time: 0.130s\n",
      "test time:  0.003s\n",
      "accuracy:   0.701\n",
      "dimensionality: 8285\n",
      "density: 1.000000\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.75      0.44      0.55        93\n",
      "     british       0.59      0.15      0.24       161\n",
      "cajun_creole       0.68      0.61      0.64       309\n",
      "     chinese       0.59      0.90      0.72       535\n",
      "    filipino       0.78      0.24      0.37       151\n",
      "      french       0.61      0.48      0.54       529\n",
      "       greek       0.81      0.52      0.63       235\n",
      "      indian       0.72      0.91      0.80       601\n",
      "       irish       0.71      0.23      0.34       133\n",
      "     italian       0.73      0.89      0.81      1568\n",
      "    jamaican       0.79      0.44      0.56       105\n",
      "    japanese       0.83      0.67      0.74       284\n",
      "      korean       0.77      0.36      0.49       166\n",
      "     mexican       0.83      0.91      0.87      1288\n",
      "    moroccan       0.78      0.46      0.58       164\n",
      "     russian       0.63      0.19      0.30        98\n",
      " southern_us       0.55      0.74      0.63       864\n",
      "     spanish       0.67      0.18      0.28       198\n",
      "        thai       0.63      0.61      0.62       308\n",
      "  vietnamese       0.72      0.21      0.32       165\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      7955\n",
      "   macro avg       0.71      0.51      0.55      7955\n",
      "weighted avg       0.71      0.70      0.68      7955\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=2019, tol=0.001,\n",
      "     verbose=0),\n",
      "        max_features=None, no...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=2019, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 16.891s\n",
      "test time:  0.006s\n",
      "accuracy:   0.797\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.77      0.59      0.67        93\n",
      "     british       0.67      0.46      0.54       161\n",
      "cajun_creole       0.77      0.72      0.74       309\n",
      "     chinese       0.80      0.86      0.83       535\n",
      "    filipino       0.75      0.61      0.67       151\n",
      "      french       0.63      0.69      0.65       529\n",
      "       greek       0.82      0.72      0.77       235\n",
      "      indian       0.87      0.91      0.89       601\n",
      "       irish       0.65      0.45      0.53       133\n",
      "     italian       0.83      0.89      0.86      1568\n",
      "    jamaican       0.82      0.71      0.77       105\n",
      "    japanese       0.84      0.70      0.76       284\n",
      "      korean       0.82      0.80      0.81       166\n",
      "     mexican       0.90      0.92      0.91      1288\n",
      "    moroccan       0.80      0.78      0.79       164\n",
      "     russian       0.62      0.46      0.53        98\n",
      " southern_us       0.71      0.81      0.76       864\n",
      "     spanish       0.69      0.48      0.57       198\n",
      "        thai       0.79      0.79      0.79       308\n",
      "  vietnamese       0.72      0.61      0.66       165\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7955\n",
      "   macro avg       0.76      0.70      0.73      7955\n",
      "weighted avg       0.79      0.80      0.79      7955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RANDOM_ST = 2019\n",
    "\n",
    "target_names = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n",
    "           'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n",
    "           'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"sag\",random_state=RANDOM_ST), \"Ridge Classifier\"),\n",
    "        (Perceptron(max_iter=50, tol=1e-3,random_state=RANDOM_ST), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3,random_state=RANDOM_ST), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100,random_state=RANDOM_ST), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "    \n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,penalty=penalty,random_state=RANDOM_ST)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,penalty=\"elasticnet\",random_state=RANDOM_ST)))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,tol=1e-3,random_state=RANDOM_ST))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\",random_state=RANDOM_ST))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=25, min_child_weight=1, missing=None, n_estimators=400,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
      "       random_state=2019, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "train time: 2393.403s\n",
      "test time:  20.284s\n",
      "accuracy:   0.791\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.86      0.60      0.71        93\n",
      "     british       0.74      0.42      0.54       161\n",
      "cajun_creole       0.79      0.69      0.74       309\n",
      "     chinese       0.80      0.86      0.83       535\n",
      "    filipino       0.71      0.58      0.64       151\n",
      "      french       0.62      0.64      0.63       529\n",
      "       greek       0.81      0.70      0.75       235\n",
      "      indian       0.86      0.91      0.88       601\n",
      "       irish       0.72      0.51      0.60       133\n",
      "     italian       0.79      0.90      0.85      1568\n",
      "    jamaican       0.85      0.67      0.75       105\n",
      "    japanese       0.84      0.70      0.76       284\n",
      "      korean       0.84      0.64      0.73       166\n",
      "     mexican       0.90      0.93      0.92      1288\n",
      "    moroccan       0.88      0.74      0.81       164\n",
      "     russian       0.67      0.37      0.47        98\n",
      " southern_us       0.70      0.80      0.75       864\n",
      "     spanish       0.66      0.48      0.56       198\n",
      "        thai       0.81      0.81      0.81       308\n",
      "  vietnamese       0.69      0.61      0.65       165\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      7955\n",
      "   macro avg       0.78      0.68      0.72      7955\n",
      "weighted avg       0.79      0.79      0.79      7955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# xgb  <- xgboost(xgbmat, max.depth = 25, \n",
    "# eta = 0.3, nround = 200, objective = \"multi:softmax\", num_class = 20)\n",
    "# https://www.kaggle.com/mohdatir/xgboost\n",
    "\n",
    "results.append(benchmark(xgb.XGBClassifier(max_depth=25, n_estimators=400, random_state=RANDOM_ST)))\n",
    "\n",
    "# objective='binary:logistic'  objective issue : \n",
    "# https://stackoverflow.com/questions/39386966/multiclass-classification-in-xgboost-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
