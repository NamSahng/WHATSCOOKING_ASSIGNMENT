{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(ing):\n",
    "    # remove numbers from ingredients\n",
    "    \n",
    "    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in ing]\n",
    "\n",
    "    \n",
    "def remove_special_chars(ing):\n",
    "    # remove certain special characters from ingredients\n",
    "   \n",
    "    ing = [[x.replace(\"-\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"&\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"'\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"''\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"%\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"!\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"(\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\")\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\",\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(\".\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in ing] \n",
    "    ing = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in ing] \n",
    "\n",
    "    return ing\n",
    "    \n",
    "    \n",
    "def make_lowercase(ing):\n",
    "    # make all letters lowercase for all ingredients\n",
    "    \n",
    "    return [[x.lower() for x in y] for y in ing]\n",
    "    \n",
    "    \n",
    "def remove_extra_whitespace(ing):\n",
    "    # removes extra whitespaces\n",
    "    \n",
    "    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in ing] \n",
    "    \n",
    "    \n",
    "def stem_words(ing):\n",
    "    # word stemming for ingredients\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    def word_by_word(strng):\n",
    "        \n",
    "        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in strng.split()])\n",
    "    \n",
    "    return [[word_by_word(x) for x in y] for y in ing] \n",
    "    \n",
    "    \n",
    "def remove_units(ing):\n",
    "    # remove certain words from ingredients\n",
    "    \n",
    "    remove_list = ['g', 'lb', 's', 'n']\n",
    "        \n",
    "    def check_word(strng):\n",
    "        \n",
    "        s = strng.split()\n",
    "        resw  = [word for word in s if word.lower() not in remove_list]\n",
    "        \n",
    "        return ' '.join(resw)\n",
    "\n",
    "    return [[check_word(x) for x in y] for y in ing] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### by all the words\n",
    "\n",
    "df=pd.read_json('./train.json')\n",
    "X = df['ingredients'].values\n",
    "Y = df['cuisine'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_lowercase(X)           # 소문자로 변환\n",
    "X = remove_numbers(X)           # 숫자 제거 \n",
    "X = remove_special_chars(X)     # 특수 문자제거\n",
    "X = remove_extra_whitespace(X)  # 추가 공백 제거\n",
    "X = remove_units(X)             # ['g', 'lb', 's', 'n'] 와 같은 단위 제거\n",
    "# X = stem_words(X)               # Lemmatization(원형화) nltk를 활용한 WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients_preprocessed'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients_as_sentence'] = df['ingredients_preprocessed'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6686\n"
     ]
    }
   ],
   "source": [
    "uniques = list(set([item for sublist in X for item in sublist]))\n",
    "print(len(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['ingredients_as_sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8524\n"
     ]
    }
   ],
   "source": [
    "for ing in uniques:\n",
    "    temp = ing.split(' ')\n",
    "    for word in temp:\n",
    "        if word not in uniques:\n",
    "            uniques.append(word)\n",
    "print(len(uniques))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819,) (7955,) (31819,) (7955,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2 ,random_state=2019, stratify = Y)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bourbon whiskey, water, simple syrup, granulated sugar, fresh mint, powdered sugar, mint sprigs\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE =  './crawl-300d-2M.vec'\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_embeddings(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 4147\n"
     ]
    }
   ],
   "source": [
    "x = np.r_[X_train, X_test]\n",
    "### movie tokenizer\n",
    "tokenizer = Tokenizer(lower=True, filters='\\n\\t')\n",
    "tokenizer.fit_on_texts(x)\n",
    "x_train = tokenizer.texts_to_sequences(X_train)\n",
    "x_test  = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\n",
    "print('vocabulary size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen: 141\n",
      "(31819, 141)\n",
      "(7955, 141)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
    "print('maxlen: {}'.format(maxlen))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV: 1775\n"
     ]
    }
   ],
   "source": [
    "def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n",
    "    embedding_matrix = np.zeros([vocab_size, dim])\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        vector = embeddings.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_size = 300\n",
    "embedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n",
    "                                     vocab_size, embedding_size)\n",
    "print('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Keras Layer that implements an Attention mechanism for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    :param kwargs:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(Attention())\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n",
    "    input_words = Input((maxlen, ))\n",
    "    x_words = Embedding(vocab_size,\n",
    "                        embedding_size,\n",
    "                        weights=[embedding_matrix],\n",
    "                        mask_zero=True,\n",
    "                        trainable=False)(input_words)\n",
    "    x_words = SpatialDropout1D(0.3)(x_words)\n",
    "    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n",
    "    x = Attention(maxlen)(x_words)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    pred = Dense(20, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_words, outputs=pred)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_trainn = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(y_trainn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 141)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 141, 300)          1244100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 141, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 141, 100)          140400    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 100)               241       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                1020      \n",
      "=================================================================\n",
      "Total params: 1,390,811\n",
      "Trainable params: 146,711\n",
      "Non-trainable params: 1,244,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "31819/31819 [==============================] - 88s 3ms/step - loss: 2.0539 - acc: 0.4088\n",
      "Epoch 2/200\n",
      "31819/31819 [==============================] - 87s 3ms/step - loss: 1.4726 - acc: 0.5625\n",
      "Epoch 3/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.3488 - acc: 0.5968\n",
      "Epoch 4/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.2842 - acc: 0.6177\n",
      "Epoch 5/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.2349 - acc: 0.6301\n",
      "Epoch 6/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.2018 - acc: 0.6429\n",
      "Epoch 7/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 1.1666 - acc: 0.6535\n",
      "Epoch 8/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.1444 - acc: 0.6585\n",
      "Epoch 9/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.1161 - acc: 0.6633\n",
      "Epoch 10/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 1.0993 - acc: 0.6688\n",
      "Epoch 11/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.0788 - acc: 0.6772\n",
      "Epoch 12/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 1.0566 - acc: 0.6811\n",
      "Epoch 13/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.0460 - acc: 0.6846\n",
      "Epoch 14/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 1.0358 - acc: 0.6876\n",
      "Epoch 15/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 1.0230 - acc: 0.6908\n",
      "Epoch 16/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 1.0101 - acc: 0.6961\n",
      "Epoch 17/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9979 - acc: 0.6999\n",
      "Epoch 18/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9856 - acc: 0.7019\n",
      "Epoch 19/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.9805 - acc: 0.7059\n",
      "Epoch 20/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9660 - acc: 0.7068\n",
      "Epoch 21/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9572 - acc: 0.7116\n",
      "Epoch 22/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.9499 - acc: 0.7111\n",
      "Epoch 23/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9421 - acc: 0.7145\n",
      "Epoch 24/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9302 - acc: 0.7166\n",
      "Epoch 25/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.9216 - acc: 0.7194\n",
      "Epoch 26/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.9093 - acc: 0.7205\n",
      "Epoch 27/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9097 - acc: 0.7264\n",
      "Epoch 28/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.9022 - acc: 0.7222\n",
      "Epoch 29/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.8914 - acc: 0.7282\n",
      "Epoch 30/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8869 - acc: 0.7268\n",
      "Epoch 31/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8769 - acc: 0.7329\n",
      "Epoch 32/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8767 - acc: 0.7308\n",
      "Epoch 33/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8679 - acc: 0.7352\n",
      "Epoch 34/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8601 - acc: 0.7367\n",
      "Epoch 35/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8562 - acc: 0.7366\n",
      "Epoch 36/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8489 - acc: 0.7408\n",
      "Epoch 37/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8474 - acc: 0.7406\n",
      "Epoch 38/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8388 - acc: 0.7406\n",
      "Epoch 39/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8367 - acc: 0.7446\n",
      "Epoch 40/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8288 - acc: 0.7454\n",
      "Epoch 41/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8250 - acc: 0.7457\n",
      "Epoch 42/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8183 - acc: 0.7476\n",
      "Epoch 43/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8163 - acc: 0.7504\n",
      "Epoch 44/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8115 - acc: 0.7498\n",
      "Epoch 45/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.8014 - acc: 0.7537\n",
      "Epoch 46/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.8012 - acc: 0.7519\n",
      "Epoch 47/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7982 - acc: 0.7534\n",
      "Epoch 48/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7913 - acc: 0.7538\n",
      "Epoch 49/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.7877 - acc: 0.7560\n",
      "Epoch 50/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7810 - acc: 0.7567\n",
      "Epoch 51/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7817 - acc: 0.7557\n",
      "Epoch 52/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.7778 - acc: 0.7565\n",
      "Epoch 53/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7749 - acc: 0.7621\n",
      "Epoch 54/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7644 - acc: 0.7611\n",
      "Epoch 55/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7667 - acc: 0.7631\n",
      "Epoch 56/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7667 - acc: 0.7628\n",
      "Epoch 57/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7541 - acc: 0.7644\n",
      "Epoch 58/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7555 - acc: 0.7651\n",
      "Epoch 59/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7520 - acc: 0.7661\n",
      "Epoch 60/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.7523 - acc: 0.7661\n",
      "Epoch 61/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7504 - acc: 0.7673\n",
      "Epoch 62/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7482 - acc: 0.7674\n",
      "Epoch 63/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7344 - acc: 0.7702\n",
      "Epoch 64/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7348 - acc: 0.7706\n",
      "Epoch 65/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7332 - acc: 0.7707\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.7276 - acc: 0.7728\n",
      "Epoch 67/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7272 - acc: 0.7725\n",
      "Epoch 68/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7214 - acc: 0.7724\n",
      "Epoch 69/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7202 - acc: 0.7735\n",
      "Epoch 70/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7207 - acc: 0.7741\n",
      "Epoch 71/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.7106 - acc: 0.7756\n",
      "Epoch 72/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.7115 - acc: 0.7771\n",
      "Epoch 73/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.7083 - acc: 0.7779\n",
      "Epoch 74/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7080 - acc: 0.7771\n",
      "Epoch 75/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7104 - acc: 0.7748\n",
      "Epoch 76/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.7059 - acc: 0.7788\n",
      "Epoch 77/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.7017 - acc: 0.7794\n",
      "Epoch 78/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7000 - acc: 0.7807\n",
      "Epoch 79/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.7005 - acc: 0.7793\n",
      "Epoch 80/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6895 - acc: 0.7825\n",
      "Epoch 81/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6877 - acc: 0.7836\n",
      "Epoch 82/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6907 - acc: 0.7824\n",
      "Epoch 83/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.6855 - acc: 0.7820\n",
      "Epoch 84/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6877 - acc: 0.7840\n",
      "Epoch 85/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6843 - acc: 0.7826\n",
      "Epoch 86/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.6815 - acc: 0.7822\n",
      "Epoch 87/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6768 - acc: 0.7850\n",
      "Epoch 88/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.6792 - acc: 0.7828\n",
      "Epoch 89/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6771 - acc: 0.7856\n",
      "Epoch 90/200\n",
      "31819/31819 [==============================] - 85s 3ms/step - loss: 0.6805 - acc: 0.7855\n",
      "Epoch 91/200\n",
      "31819/31819 [==============================] - 85s 3ms/step - loss: 0.6746 - acc: 0.7873\n",
      "Epoch 92/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6720 - acc: 0.7852\n",
      "Epoch 93/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6728 - acc: 0.7867\n",
      "Epoch 94/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6627 - acc: 0.7896\n",
      "Epoch 95/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6632 - acc: 0.7897\n",
      "Epoch 96/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6589 - acc: 0.7921\n",
      "Epoch 97/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6631 - acc: 0.7881\n",
      "Epoch 98/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6625 - acc: 0.7907\n",
      "Epoch 99/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6577 - acc: 0.7933\n",
      "Epoch 100/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6587 - acc: 0.7900\n",
      "Epoch 101/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6594 - acc: 0.7891\n",
      "Epoch 102/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6470 - acc: 0.7926\n",
      "Epoch 103/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6467 - acc: 0.7958\n",
      "Epoch 104/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.6469 - acc: 0.7931\n",
      "Epoch 105/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6521 - acc: 0.7921\n",
      "Epoch 106/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6511 - acc: 0.7935\n",
      "Epoch 107/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6456 - acc: 0.7933\n",
      "Epoch 108/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6466 - acc: 0.7969\n",
      "Epoch 109/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6430 - acc: 0.7946\n",
      "Epoch 110/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6387 - acc: 0.7962\n",
      "Epoch 111/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.6356 - acc: 0.7951\n",
      "Epoch 112/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.6356 - acc: 0.7969\n",
      "Epoch 113/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.6348 - acc: 0.7955\n",
      "Epoch 114/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.6372 - acc: 0.7978\n",
      "Epoch 115/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6298 - acc: 0.7985\n",
      "Epoch 116/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6349 - acc: 0.7961\n",
      "Epoch 117/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6311 - acc: 0.7958\n",
      "Epoch 118/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6251 - acc: 0.7992\n",
      "Epoch 119/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6274 - acc: 0.7980\n",
      "Epoch 120/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6272 - acc: 0.7980\n",
      "Epoch 121/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6180 - acc: 0.8016\n",
      "Epoch 122/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6228 - acc: 0.8015\n",
      "Epoch 123/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6195 - acc: 0.8020\n",
      "Epoch 124/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6224 - acc: 0.8015\n",
      "Epoch 125/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6178 - acc: 0.8004\n",
      "Epoch 126/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6189 - acc: 0.8033\n",
      "Epoch 127/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6164 - acc: 0.8029\n",
      "Epoch 128/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6184 - acc: 0.8020\n",
      "Epoch 129/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6145 - acc: 0.8000\n",
      "Epoch 130/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.6096 - acc: 0.8047\n",
      "Epoch 131/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.6075 - acc: 0.8044\n",
      "Epoch 132/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6108 - acc: 0.8037\n",
      "Epoch 133/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6126 - acc: 0.8051\n",
      "Epoch 134/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6072 - acc: 0.8033\n",
      "Epoch 135/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6032 - acc: 0.8048\n",
      "Epoch 136/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5998 - acc: 0.8055\n",
      "Epoch 137/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.6084 - acc: 0.8037\n",
      "Epoch 138/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5994 - acc: 0.8052\n",
      "Epoch 139/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6039 - acc: 0.8035\n",
      "Epoch 140/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6077 - acc: 0.8051\n",
      "Epoch 141/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.6051 - acc: 0.8031\n",
      "Epoch 142/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.6031 - acc: 0.8051\n",
      "Epoch 143/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5994 - acc: 0.8080\n",
      "Epoch 144/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5983 - acc: 0.8078\n",
      "Epoch 145/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5985 - acc: 0.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5965 - acc: 0.8079\n",
      "Epoch 147/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5987 - acc: 0.8080\n",
      "Epoch 148/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5979 - acc: 0.8047\n",
      "Epoch 149/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5892 - acc: 0.8106\n",
      "Epoch 150/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5940 - acc: 0.8066\n",
      "Epoch 151/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5844 - acc: 0.8122\n",
      "Epoch 152/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5950 - acc: 0.8094\n",
      "Epoch 153/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5911 - acc: 0.8079\n",
      "Epoch 154/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5849 - acc: 0.8123\n",
      "Epoch 155/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5892 - acc: 0.8090\n",
      "Epoch 156/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5844 - acc: 0.8118\n",
      "Epoch 157/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.5848 - acc: 0.8109\n",
      "Epoch 158/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5841 - acc: 0.8108\n",
      "Epoch 159/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5835 - acc: 0.8071\n",
      "Epoch 160/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5828 - acc: 0.8110\n",
      "Epoch 161/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5792 - acc: 0.8109\n",
      "Epoch 162/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5805 - acc: 0.8143\n",
      "Epoch 163/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5757 - acc: 0.8142\n",
      "Epoch 164/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5808 - acc: 0.8125\n",
      "Epoch 165/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5753 - acc: 0.8142\n",
      "Epoch 166/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5782 - acc: 0.8114\n",
      "Epoch 167/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.5763 - acc: 0.8139\n",
      "Epoch 168/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5677 - acc: 0.8157\n",
      "Epoch 169/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5742 - acc: 0.8136\n",
      "Epoch 170/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5720 - acc: 0.8133\n",
      "Epoch 171/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5688 - acc: 0.8177\n",
      "Epoch 172/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5670 - acc: 0.8151\n",
      "Epoch 173/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5731 - acc: 0.8137\n",
      "Epoch 174/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5702 - acc: 0.8139\n",
      "Epoch 175/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5718 - acc: 0.8151\n",
      "Epoch 176/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5737 - acc: 0.8157\n",
      "Epoch 177/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5680 - acc: 0.8143\n",
      "Epoch 178/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5640 - acc: 0.8176\n",
      "Epoch 179/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.5638 - acc: 0.8164\n",
      "Epoch 180/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5697 - acc: 0.8136\n",
      "Epoch 181/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.5586 - acc: 0.8177\n",
      "Epoch 182/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5622 - acc: 0.8164\n",
      "Epoch 183/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5646 - acc: 0.8162\n",
      "Epoch 184/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5599 - acc: 0.8181\n",
      "Epoch 185/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5664 - acc: 0.8166\n",
      "Epoch 186/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5623 - acc: 0.8177\n",
      "Epoch 187/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5635 - acc: 0.8184\n",
      "Epoch 188/200\n",
      "31819/31819 [==============================] - 79s 2ms/step - loss: 0.5600 - acc: 0.8172\n",
      "Epoch 189/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5692 - acc: 0.8172\n",
      "Epoch 190/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5621 - acc: 0.8156\n",
      "Epoch 191/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5634 - acc: 0.8196\n",
      "Epoch 192/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.5550 - acc: 0.8189\n",
      "Epoch 193/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5634 - acc: 0.8184\n",
      "Epoch 194/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5658 - acc: 0.8172\n",
      "Epoch 195/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5666 - acc: 0.8146\n",
      "Epoch 196/200\n",
      "31819/31819 [==============================] - 84s 3ms/step - loss: 0.5576 - acc: 0.8179\n",
      "Epoch 197/200\n",
      "31819/31819 [==============================] - 83s 3ms/step - loss: 0.5591 - acc: 0.8187\n",
      "Epoch 198/200\n",
      "31819/31819 [==============================] - 80s 3ms/step - loss: 0.5549 - acc: 0.8191\n",
      "Epoch 199/200\n",
      "31819/31819 [==============================] - 81s 3ms/step - loss: 0.5566 - acc: 0.8183\n",
      "Epoch 200/200\n",
      "31819/31819 [==============================] - 82s 3ms/step - loss: 0.5560 - acc: 0.8181\n"
     ]
    }
   ],
   "source": [
    "model = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "save_file = './model_attention.h5'\n",
    "history = model.fit(x_train, y_trainn,\n",
    "                    epochs=200, verbose=1,\n",
    "                    batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7955,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred = y_pred.argmax(axis=1).astype(int)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Lec.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6898805782526712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.48      0.40      0.44        93\n",
      "     british       0.35      0.30      0.32       161\n",
      "cajun_creole       0.69      0.60      0.64       309\n",
      "     chinese       0.72      0.78      0.75       535\n",
      "    filipino       0.55      0.46      0.50       151\n",
      "      french       0.51      0.54      0.52       529\n",
      "       greek       0.65      0.64      0.65       235\n",
      "      indian       0.80      0.83      0.81       601\n",
      "       irish       0.46      0.31      0.37       133\n",
      "     italian       0.78      0.80      0.79      1568\n",
      "    jamaican       0.52      0.43      0.47       105\n",
      "    japanese       0.67      0.58      0.62       284\n",
      "      korean       0.66      0.63      0.64       166\n",
      "     mexican       0.85      0.85      0.85      1288\n",
      "    moroccan       0.64      0.62      0.63       164\n",
      "     russian       0.35      0.27      0.30        98\n",
      " southern_us       0.57      0.69      0.63       864\n",
      "     spanish       0.54      0.42      0.48       198\n",
      "        thai       0.71      0.68      0.69       308\n",
      "  vietnamese       0.57      0.50      0.54       165\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      7955\n",
      "   macro avg       0.60      0.57      0.58      7955\n",
      "weighted avg       0.69      0.69      0.69      7955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n",
    "           'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n",
    "           'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "score = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(classification_report(y_test, y_pred ,target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
